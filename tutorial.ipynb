{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from tempfile import NamedTemporaryFile\n",
    "from IPython.display import HTML\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import six\n",
    "import tensorflow as tf\n",
    "\n",
    "plt.style.use('seaborn-talk')\n",
    "sns.set_context(\"talk\", font_scale=1.4)\n",
    "# sns.set_palette(\"colorblind\")\n",
    "sess = ed.get_session()\n",
    "\n",
    "sns.palplot(sns.color_palette())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this can be done only before using Edward\n",
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VIDEO_TAG = \"\"\"<video controls>\n",
    " <source src=\"data:video/x-m4v;base64,{0}\" type=\"video/mp4\">\n",
    " Your browser does not support the video tag.\n",
    "</video>\"\"\"\n",
    "\n",
    "def anim_to_html(anim):\n",
    "    if not hasattr(anim, '_encoded_video'):\n",
    "        with NamedTemporaryFile(suffix='.mp4') as f:\n",
    "            anim.save(f.name, fps=20, extra_args=['-vcodec', 'libx264'])\n",
    "            video = open(f.name, \"rb\").read()\n",
    "        anim._encoded_video = video.encode(\"base64\")\n",
    "    \n",
    "    return VIDEO_TAG.format(anim._encoded_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# VIDEO_TAG = \"\"\"<video controls>\n",
    "#  <source src=\"data:video/x-webm;base64,{0}\" type=\"video/webm\">\n",
    "#  Your browser does not support the video tag.\n",
    "# </video>\"\"\"\n",
    "\n",
    "# def anim_to_html(anim):\n",
    "#     if not hasattr(anim, '_encoded_video'):\n",
    "#         with NamedTemporaryFile(suffix='.webm') as f:\n",
    "#             anim.save(f.name, fps=12,\n",
    "#                       extra_args=['-vcodec', 'libvpx',\n",
    "#                                   '-g', '120',\n",
    "#                                   '-level', '216',\n",
    "#                                   '-profile', '0',\n",
    "#                                   '-qmax', '42',\n",
    "#                                   '-qmin', '10', \n",
    "#                                   '-rc_buf_aggressivity', '0.95',\n",
    "#                                   '-vb', '2M'])\n",
    "#             video = open(f.name, \"rb\").read()\n",
    "#         anim._encoded_video = video.encode(\"base64\")\n",
    "\n",
    "#     return VIDEO_TAG.format(anim._encoded_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_animation(anim):\n",
    "    plt.close(anim._fig)\n",
    "    return HTML(anim_to_html(anim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Bernoulli, Beta, Empirical, Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100  # number of coin flip observations in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_fair_dataset(N):\n",
    "    pheads = tf.constant(0.5)\n",
    "    c = Bernoulli(probs=pheads, sample_shape=N)\n",
    "    return sess.run([pheads, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_unfair_dataset(N):\n",
    "    pheads = tf.constant(0.05)\n",
    "    c = Bernoulli(probs=pheads, sample_shape=N)\n",
    "    return sess.run([pheads, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(N):\n",
    "    pheads = Uniform(low=0.0, high=1.0)\n",
    "    c = Bernoulli(probs=pheads, sample_shape=N)\n",
    "    return sess.run([pheads, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(-0.2, 1.2, 0.001)\n",
    "plt.plot(*sess.run([x, Uniform(low=0.0, high=1.0).prob(x)]));\n",
    "#plt.plot(*sess.run([x, Beta(concentration1=1.0, concentration0=1.0).prob(x)]));\n",
    "plt.ylim((-0.2, 1.2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "pheads_true, c_train = build_fair_dataset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheads_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(c_train == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(c_train == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pheads_fair = Beta(concentration1=1000.0, concentration0=1000.0)  # blue\n",
    "pheads_unfair = Beta(concentration1=0.1, concentration0=0.1)  # green\n",
    "pheads_unknown = Beta(concentration1=1.0, concentration0=1.0)  # red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(0.0, 1.0, 0.001)\n",
    "plt.plot(*sess.run([x, pheads_fair.prob(x)]));\n",
    "plt.plot(*sess.run([x, pheads_unfair.prob(x)]));\n",
    "plt.plot(*sess.run([x, pheads_unknown.prob(x)]));\n",
    "plt.axvline(x=pheads_true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL\n",
    "pheads = pheads_unknown\n",
    "c = Bernoulli(probs=pheads, sample_shape=N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exact Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "pheads_cond = ed.complete_conditional(pheads)\n",
    "pheads_post = ed.copy(pheads_cond, {c: c_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run({key: val for\n",
    "          key, val in six.iteritems(pheads_post.parameters)\n",
    "          if isinstance(val, tf.Tensor)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "mean, stddev = sess.run([pheads_post.mean(), pheads_post.stddev()])\n",
    "print(\"Exact posterior mean:\")\n",
    "print(mean)\n",
    "print(\"Exact posterior std:\")\n",
    "print(stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(0.0, 1.0, 0.001)\n",
    "plt.plot(*sess.run([x, pheads.prob(x)]));  # blue\n",
    "plt.plot(*sess.run([x, pheads_post.prob(x)]));  # green\n",
    "plt.axvline(x=pheads_true);  # blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this can take a minute\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim=(-0.05, 1.05), ylim=(-1.0, 11.0))\n",
    "\n",
    "def go(pheads_prior, sample_shape, c_train):\n",
    "    # MODEL\n",
    "    c = Bernoulli(probs=pheads_prior,\n",
    "                  sample_shape=sample_shape)\n",
    "    # INFERENCE\n",
    "    pheads_cond = ed.complete_conditional(pheads_prior)\n",
    "    pheads_post = ed.copy(pheads_cond, {c: c_train[:sample_shape]})\n",
    "    \n",
    "    # CRITICISM\n",
    "    ax.plot(*sess.run([x, pheads_post.prob(x)]));\n",
    "    \n",
    "    # RECURSION\n",
    "    if len(c_train[sample_shape:]) >= sample_shape:\n",
    "        go(pheads_post, sample_shape, c_train[sample_shape:])\n",
    "\n",
    "pheads_prior = Beta(concentration1=1.0, concentration0=1.0)\n",
    "ax.plot(*sess.run([x, pheads_prior.prob(x)]));  # blue\n",
    "plt.axvline(x=pheads_true);  # blue\n",
    "go(pheads_prior, 33, c_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCMC: Metropolis Hastings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "T = 10000  # number of empirical samples\n",
    "q_pheads = Empirical(params=tf.Variable(tf.ones([T])*.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "proposal_pheads = Beta(concentration1=1.0,\n",
    "                       concentration0=1.0)\n",
    "inference = ed.MetropolisHastings(latent_vars={pheads: q_pheads},\n",
    "                                  proposal_vars={pheads: proposal_pheads},\n",
    "                                  data={c: c_train})\n",
    "inference.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "mean, stddev = sess.run([q_pheads.mean(), q_pheads.stddev()])\n",
    "print(\"Inferred posterior mean:\")\n",
    "print(mean)\n",
    "print(\"Inferred posterior std:\")\n",
    "print(stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(q_pheads.params.eval());\n",
    "plt.axhline(y=pheads_true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lags(x):\n",
    "    mean = tf.reduce_mean(x)\n",
    "    var = tf.cast(tf.size(x) - 1, tf.float32) * tf.reduce_mean(tf.square(x - mean))\n",
    "    ret = tf.map_fn(lambda k: tf.cond(tf.equal(k, 0),\n",
    "                                      lambda: var,\n",
    "                                      lambda: tf.reduce_sum((x[:-k] - mean) * (x[k:] - mean))),\n",
    "                    tf.range(0, tf.size(x)),\n",
    "                    dtype=tf.float32)\n",
    "    return ret / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lags(q_pheads.params).eval());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(0.0, 1.0, 0.001)\n",
    "plt.plot(*sess.run([x, pheads.prob(x)]));  # blue\n",
    "plt.plot(*sess.run([x, pheads_cond.prob(x)],  # green\n",
    "                   {c: c_train}));\n",
    "plt.hist(q_pheads.params.eval(),  # red\n",
    "         bins=100, range=(0.0, 1.0),\n",
    "         normed=True);\n",
    "plt.axvline(x=pheads_true);  # blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCMC: Gibbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "T = 10000  # number of empirical samples\n",
    "q_pheads = Empirical(params=tf.Variable(tf.ones([T])*.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "inference = ed.Gibbs(latent_vars={pheads: q_pheads},\n",
    "                     data={c: c_train})\n",
    "inference.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "mean, stddev = sess.run([q_pheads.mean(), q_pheads.stddev()])\n",
    "print(\"Inferred posterior mean:\")\n",
    "print(mean)\n",
    "print(\"Inferred posterior std:\")\n",
    "print(stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(q_pheads.params.eval());\n",
    "plt.axhline(y=pheads_true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lags(q_pheads.params).eval());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(0.0, 1.0, 0.001)\n",
    "plt.plot(*sess.run([x, pheads.prob(x)]));  # blue\n",
    "plt.plot(*sess.run([x, pheads_cond.prob(x)],  # green\n",
    "                   {c: c_train}));\n",
    "plt.hist(q_pheads.params.eval(),  # red\n",
    "         bins=100, range=(0.0, 1.0),\n",
    "         normed=True);\n",
    "plt.axvline(x=pheads_true);  # blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCMC: Hamiltonian Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "T = 10000  # number of empirical samples\n",
    "q_pheads = Empirical(params=tf.Variable(tf.ones([T])*.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "inference = ed.HMC(latent_vars={pheads: q_pheads},\n",
    "                   data={c: c_train})\n",
    "inference.run(step_size=1.0 / N, n_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "mean, stddev = sess.run([q_pheads.mean(), q_pheads.stddev()])\n",
    "print(\"Inferred posterior mean:\")\n",
    "print(mean)\n",
    "print(\"Inferred posterior std:\")\n",
    "print(stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(q_pheads.params.eval());\n",
    "plt.axhline(y=pheads_true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lags(q_pheads.params).eval());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(0.0, 1.0, 0.001)\n",
    "plt.plot(*sess.run([x, pheads.prob(x)]));  # blue\n",
    "plt.plot(*sess.run([x, pheads_cond.prob(x)],  # green\n",
    "                   {c: c_train}));\n",
    "plt.hist(q_pheads.params.eval(),  # red\n",
    "         bins=100, range=(0.0, 1.0),\n",
    "         normed=True);\n",
    "plt.axvline(x=pheads_true);  # blue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variational Inference (VI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "q_pheads_concentration1 = tf.nn.softplus(tf.Variable(tf.random_normal([])))\n",
    "# q_pheads_concentration1 = tf.nn.softplus(tf.Variable(51 + tf.random_normal([])))\n",
    "q_pheads_concentration0 = tf.nn.softplus(tf.Variable(tf.random_normal([])))\n",
    "# q_pheads_concentration0 = tf.nn.softplus(tf.Variable(51 + tf.random_normal([])))\n",
    "q_pheads = Beta(concentration1=q_pheads_concentration1,\n",
    "                concentration0=q_pheads_concentration0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(-5.0, 5.0, 0.001)\n",
    "plt.plot(*sess.run([x, tf.nn.softplus(x)]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "inference = ed.KLqp(latent_vars={pheads: q_pheads},\n",
    "                    data={c: c_train})\n",
    "inference.run(n_samples=20, n_iter=1000)\n",
    "#inference.run(n_samples=1000, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# T = 10000  # number of empirical samples\n",
    "# q_pheads_samples = sess.run(q_pheads.sample(sample_shape=T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plt.plot(q_pheads_samples);\n",
    "# plt.axhline(y=pheads_true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run({key: val for\n",
    "          key, val in six.iteritems(q_pheads.parameters)\n",
    "          if isinstance(val, tf.Tensor)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(*sess.run([x, pheads.prob(x)]));  # blue\n",
    "plt.plot(*sess.run([x, pheads_cond.prob(x)],  # green\n",
    "                   {c: c_train}));\n",
    "plt.plot(*sess.run([x, q_pheads.prob(x)]));  # red\n",
    "# plt.hist(q_pheads_samples,  # red\n",
    "#          bins=100, range=(0.0, 1.0),\n",
    "#          normed=True);\n",
    "plt.axvline(x=pheads_true);  # blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "mean, stddev = sess.run([q_pheads.mean(), q_pheads.stddev()])\n",
    "print(\"Inferred posterior mean:\")\n",
    "print(mean)\n",
    "print(\"Inferred posterior std:\")\n",
    "print(stddev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B/... Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Beta, Bernoulli, Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(N, concentration1=10.0, concentration0=5.0):\n",
    "    pheads = Beta(concentration1=concentration1,\n",
    "                  concentration0=concentration0)\n",
    "    c = Bernoulli(probs=pheads, sample_shape=N)\n",
    "    return sess.run([pheads, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "NN = [50, 90, 10, 1000]  # number of flips per coin\n",
    "pheads_true, c_train = zip(*[build_dataset(N) for N in NN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pheads_true  # true coin biases (usually unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.range(0.0, 1.0, 0.001)\n",
    "plt.plot(*sess.run([x, Beta(concentration1=10.0,\n",
    "                            concentration0=5.0).prob(x)]));\n",
    "[plt.axvline(x=pheads_true[i], c=sns.color_palette()[i]) for i, _ in enumerate(pheads_true)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL\n",
    "pheads = [Beta(concentration1=10.0, concentration0=5.0) for _ in NN]\n",
    "c = [Bernoulli(probs=pheads[i], sample_shape=N) for i, N in enumerate(NN)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "T = 10000  # number of empirical samples\n",
    "q_pheads = [Empirical(params=tf.Variable(tf.ones([T])*.5)) for _ in NN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "# this will take a couple of minutes\n",
    "inference = ed.HMC(latent_vars=dict(zip(pheads, q_pheads)),\n",
    "                   data=dict(zip(c, c_train)))\n",
    "inference.run(step_size=1.0 / sum(NN), n_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "[plt.plot(q_pheads[i].params.eval(), alpha=0.7) for i, _ in enumerate(NN)];\n",
    "[plt.axhline(y=pheads_true[i], c=sns.color_palette()[i]) for i, _ in enumerate(NN)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lags(x):\n",
    "    mean = tf.reduce_mean(x)\n",
    "    var = tf.cast(tf.size(x) - 1, tf.float32) * tf.reduce_mean(tf.square(x - mean))\n",
    "    ret = tf.map_fn(lambda k: tf.cond(tf.equal(k, 0),\n",
    "                                      lambda: var,\n",
    "                                      lambda: tf.reduce_sum((x[:-k] - mean) * (x[k:] - mean))),\n",
    "                    tf.range(0, tf.size(x)),\n",
    "                    dtype=tf.float32)\n",
    "    return ret / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[plt.plot(lags(q_pheads[i].params).eval(), alpha=0.7) for i, _ in enumerate(NN)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.range(0.0, 1.0, 0.001)\n",
    "[plt.hist(q_pheads[i].params.eval(),\n",
    "          bins=100, range=(0.0, 1.0),\n",
    "          normed=True, alpha=0.7) for i, _ in enumerate(NN)];\n",
    "[plt.axvline(x=pheads_true[i], c=sns.color_palette()[i]) for i, _ in enumerate(NN)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# lift of coin 2 over coin 1 (at least %)\n",
    "# (at least) by what % is coin 2 more biased towards heads than coin 1?\n",
    "def get_prob(lift, samples_pheads1, samples_pheads2):\n",
    "    return tf.map_fn(lambda l: tf.reduce_mean(tf.to_float(tf.greater(\n",
    "        (samples_pheads2 - samples_pheads1) / samples_pheads1, l))), lift)\n",
    "\n",
    "x = tf.range(-1.0, 1.0, 0.001)\n",
    "def plot_prob(ax, i, j):\n",
    "    ax.plot(*sess.run([x*100, get_prob(x,\n",
    "                                       q_pheads[i].params[int(T/2):],\n",
    "                                       q_pheads[j].params[int(T/2):])]))\n",
    "    ax.set_xlabel('Lift of coin {} over coin {} (at least %)'.format(j, i))\n",
    "    ax.set_ylabel('Probability')\n",
    "    ax.set_title('Lift')\n",
    "\n",
    "fig, ax = plt.subplots(nrows=int(len(NN) * (len(NN) - 1) / 2),\n",
    "                       ncols=1,\n",
    "                       figsize=(12.0, 2.0 * len(NN) * (len(NN) - 1)))\n",
    "\n",
    "k = 0\n",
    "for i in range(0, len(NN)):\n",
    "    for j in range(i + 1, len(NN)):\n",
    "        plot_prob(ax[k], i, j)\n",
    "        k += 1\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For The Adventurous: Hierarchical Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# HIERARCHICAL FORWARD MODEL\n",
    "beta_mean = Uniform(low=0.0, high=1.0)  # prior for beta mean\n",
    "beta_variance = Uniform(low=0.0, high=beta_mean * (1.0 - beta_mean))  # prior for beta variance\n",
    "concentration1 = beta_mean * (((1.0 - beta_mean) * beta_mean) / beta_variance - 1.0)\n",
    "concentration0 = concentration1 * (-1.0 + 1.0 / beta_mean)\n",
    "pheads = [Beta(concentration1=concentration1,\n",
    "               concentration0=concentration0) for _ in NN]\n",
    "c = [Bernoulli(probs=pheads[i], sample_shape=N) for i, N in enumerate(NN)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "T = 50000  # number of empirical samples\n",
    "q_beta_mean = Empirical(params=tf.Variable(tf.ones([T])*.5))\n",
    "q_beta_variance = Empirical(params=tf.Variable(tf.ones([T])*.01))\n",
    "q_pheads = [Empirical(params=tf.Variable(tf.ones([T])*.5)) for _ in NN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "# this can take 20 minutes\n",
    "inference = ed.HMC(latent_vars=dict(zip(pheads, q_pheads) +\n",
    "                                    [(beta_mean, q_beta_mean),\n",
    "                                     (beta_variance, q_beta_variance)]),\n",
    "                   data=dict(zip(c, c_train)))\n",
    "inference.run(step_size=0.1 / sum(NN), n_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "[plt.plot(q_pheads[i].params.eval(), alpha=0.7) for i, _ in enumerate(NN)];\n",
    "[plt.axhline(y=pheads_true[i], c=sns.color_palette()[i]) for i, _ in enumerate(NN)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lags(x):\n",
    "    mean = tf.reduce_mean(x)\n",
    "    var = tf.cast(tf.size(x) - 1, tf.float32) * tf.reduce_mean(tf.square(x - mean))\n",
    "    ret = tf.map_fn(lambda k: tf.cond(tf.equal(k, 0),\n",
    "                                      lambda: var,\n",
    "                                      lambda: tf.reduce_sum((x[:-k] - mean) * (x[k:] - mean))),\n",
    "                    tf.range(0, tf.size(x)),\n",
    "                    dtype=tf.float32)\n",
    "    return ret / var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[plt.plot(lags(q_pheads[i].params).eval(), alpha=0.7) for i, _ in enumerate(NN)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.range(0.0, 1.0, 0.001)\n",
    "[plt.hist(q_pheads[i].params.eval(),\n",
    "          bins=100, range=(0.0, 1.0),\n",
    "          normed=True, alpha=0.7) for i, _ in enumerate(NN)];\n",
    "[plt.axvline(x=pheads_true[i], c=sns.color_palette()[i]) for i, _ in enumerate(NN)];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.range(-5.0, 5.0, 0.001)\n",
    "plt.plot(*sess.run([x, Normal(loc=tf.ones(1) * 0.0,                # blue\n",
    "                              scale=tf.ones(1) * 1.0).prob(x)]));\n",
    "plt.plot(*sess.run([x, Normal(loc=tf.ones(1) * 2.0,                # green\n",
    "                              scale=tf.ones(1) * 1.0).prob(x)]));\n",
    "plt.plot(*sess.run([x, Normal(loc=tf.ones(1) * 0.0,                # red\n",
    "                              scale=tf.ones(1) * 2.0).prob(x)]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "N1 = 10  # number of training data points in first batch\n",
    "N2 = 90  # number of training data points in second batch\n",
    "Np = 10  # number of test data points\n",
    "D = 1  # number of features\n",
    "\n",
    "weights_true = sess.run(Normal(loc=tf.ones(D) * 2.0,\n",
    "                               scale=tf.ones(D) * 0.1))  # unknown true weights\n",
    "intercept_true = sess.run(Normal(loc=tf.zeros(1),\n",
    "                                 scale=tf.ones(1)))  # unknown true intercept\n",
    "noise_true = 0.35  # unknown true amount of noise\n",
    "\n",
    "def build_dataset(N):\n",
    "    x = Normal(loc=tf.zeros([N, D]), scale=tf.ones([N, D]))\n",
    "    y = Normal(loc=ed.dot(x, weights_true) + intercept_true, scale=noise_true)\n",
    "    return sess.run([x, y])\n",
    "\n",
    "x_train1, y_train1 = build_dataset(N1)\n",
    "x_train2, y_train2 = build_dataset(N2)\n",
    "x_test, y_test = build_dataset(Np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_train1, y_train1, s=20.0);  # blue\n",
    "# plt.scatter(x_train2, y_train2, s=20.0);  # green\n",
    "plt.scatter(x_test, y_test, s=20.0,\n",
    "            color=sns.color_palette().as_hex()[2]);  # red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Little Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Empirical, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL\n",
    "x = tf.placeholder(tf.float32, [N1, D])\n",
    "weights = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "intercept = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "y = Normal(loc=ed.dot(x, weights) + intercept,\n",
    "           scale=tf.ones(N1) * 0.01)  # with little noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "T = 10000  # number of empirical samples\n",
    "q_weights = Empirical(params=tf.Variable(tf.zeros([T, D])))\n",
    "q_intercept = Empirical(params=tf.Variable(tf.zeros([T, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "inference = ed.HMC(latent_vars={weights: q_weights,\n",
    "                                intercept: q_intercept},\n",
    "                   data={x: x_train1,\n",
    "                         y: y_train1})\n",
    "inference.run(step_size=0.01 / N1, n_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "plt.scatter(x_train1, y_train1, s=20.0);  # blue\n",
    "plt.scatter(x_test, y_test, s=20.0,\n",
    "            color=sns.color_palette().as_hex()[2]);  # red\n",
    "\n",
    "xp = tf.placeholder(tf.float32, [2, D])\n",
    "[plt.plot(np.linspace(-4.0, 4.0, 2),\n",
    "          sess.run(ed.dot(xp, q_weights.params[t]) + q_intercept.params[t],\n",
    "                   {xp: np.linspace(-4.0, 4.0, 2)[:, np.newaxis]}),\n",
    "          color='black', alpha=0.1)\n",
    " for t in range(int(T/2), T, int(T/100))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_post = ed.copy(y, {weights: q_weights,\n",
    "                     intercept: q_intercept})\n",
    "# this is equivalent to\n",
    "# y_post = Normal(loc=ed.dot(x, q_weights) + q_intercept,\n",
    "#                 scale=tf.ones(N1) * 0.01)\n",
    "# ed.copy works for us only because Np=N1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={x: x_test, y_post: y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={x: x_test, y_post: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# that's not bad, but the model is way too overconfident."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL\n",
    "x = tf.placeholder(tf.float32, [N1, D])\n",
    "weights = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "intercept = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "y = Normal(loc=ed.dot(x, weights) + intercept,\n",
    "           scale=tf.ones(N1))  # with more noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "T = 10000  # number of empirical samples\n",
    "q_weights = Empirical(params=tf.Variable(tf.zeros([T, D])))\n",
    "q_intercept = Empirical(params=tf.Variable(tf.zeros([T, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "inference = ed.HMC(latent_vars={weights: q_weights,\n",
    "                                intercept: q_intercept},\n",
    "                   data={x: x_train1,\n",
    "                         y: y_train1})\n",
    "inference.run(step_size=0.01 / N1, n_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "plt.scatter(x_train1, y_train1, s=20.0);\n",
    "plt.scatter(x_test, y_test, s=20.0);\n",
    "\n",
    "xp = tf.placeholder(tf.float32, [2, D])\n",
    "[plt.plot(np.linspace(-4.0, 4.0, 2),\n",
    "          sess.run(ed.dot(xp, q_weights.params[t]) + q_intercept.params[t],\n",
    "                   {xp: np.linspace(-4.0, 4.0, 2)[:, np.newaxis]}),\n",
    "          color='black', alpha=0.1)\n",
    " for t in range(int(T/2), T, int(T/100))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_post = ed.copy(y, {weights: q_weights,\n",
    "                     intercept: q_intercept})\n",
    "# this is equivalent to\n",
    "# y_post = Normal(loc=ed.dot(x, q_weights) + q_intercept,\n",
    "#                 scale=tf.ones(N1))\n",
    "# ed.copy works for us only because Np=N1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={x: x_test, y_post: y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={x: x_test, y_post: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# too much noise!\n",
    "# the model could be more confident.\n",
    "# what is the right amount of noise?\n",
    "# what do we do in these cases?\n",
    "# we put a prior on the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior On Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import InverseGamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.range(0.0, 1.0, 0.001)\n",
    "plt.plot(*sess.run([x, InverseGamma(concentration=5.0, rate=1.0).prob(x)]));  # blue\n",
    "plt.plot(*sess.run([x, InverseGamma(concentration=3.0, rate=1.0).prob(x)]));  # green\n",
    "plt.plot(*sess.run([x, InverseGamma(concentration=1.0, rate=1.0).prob(x)]));  # red\n",
    "plt.axvline(x=noise_true**2);  # blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL\n",
    "x = tf.placeholder(tf.float32, [N1, D])\n",
    "weights = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "intercept = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "var = InverseGamma(concentration=5.0, rate=1.0)  # noise prior\n",
    "y = Normal(loc=ed.dot(x, weights) + intercept,\n",
    "           scale=tf.ones(N1) * tf.sqrt(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "T = 10000  # number of empirical samples\n",
    "\n",
    "q_weights = Empirical(params=tf.Variable(tf.zeros([T, D])))\n",
    "q_intercept = Empirical(params=tf.Variable(tf.zeros([T, 1])))\n",
    "q_var = Empirical(params=tf.Variable(tf.ones([T])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "inference = ed.HMC(latent_vars={weights: q_weights,\n",
    "                                intercept: q_intercept,\n",
    "                                var: q_var},\n",
    "                   data={x: x_train1,\n",
    "                         y: y_train1})\n",
    "inference.run(step_size=0.1 / N1, n_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "plt.hist(q_var.params[int(T/2):].eval(),\n",
    "         bins=100, normed=True);\n",
    "plt.axvline(x=noise_true**2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(q_weights.params[int(T/2):].eval(),\n",
    "         bins=100, normed=True);\n",
    "plt.axvline(x=weights_true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(q_intercept.params[int(T/2):].eval(),\n",
    "         bins=100, normed=True);\n",
    "plt.axvline(x=intercept_true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_train1, y_train1, s=20.0);  # blue\n",
    "plt.scatter(x_test, y_test, s=20.0,\n",
    "            color=sns.color_palette().as_hex()[2]);  # red\n",
    "\n",
    "xp = tf.placeholder(tf.float32, [2, D])\n",
    "[plt.plot(np.linspace(-4.0, 4.0, 2),\n",
    "          sess.run(ed.dot(xp, q_weights.params[t]) + q_intercept.params[t],\n",
    "                   {xp: np.linspace(-4.0, 4.0, 2)[:, np.newaxis]}),\n",
    "          color='black', alpha=0.1)\n",
    "for t in range(int(T/2), T, int(T/100))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_post = ed.copy(y, {weights: q_weights,\n",
    "                     intercept: q_intercept,\n",
    "                     var: q_var})\n",
    "# this is equivalent to\n",
    "# y_post = Normal(loc=ed.dot(x, q_weights) + q_intercept,\n",
    "#                 scale=tf.ones(N1) * tf.sqrt(q_var))\n",
    "# ed.copy works for us only because Np=N1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={x: x_test, y_post: y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={x: x_test, y_post: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For The Adventurous: Use The Posterior for Batch 1 as The Prior for Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL\n",
    "x = tf.placeholder(tf.float32, [N2, D])\n",
    "weights = q_weights  # posterior becomes prior\n",
    "intercept = q_intercept  # posterior becomes prior\n",
    "var = q_var  # posterior becomes prior\n",
    "y = Normal(loc=ed.dot(x, weights) + intercept,\n",
    "           scale=tf.ones(N2) * tf.sqrt(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "T = 10000  # number of empirical samples\n",
    "\n",
    "q_weights2 = Empirical(params=tf.Variable(tf.zeros([T, D])))\n",
    "q_intercept2 = Empirical(params=tf.Variable(tf.zeros([T, 1])))\n",
    "q_var2 = Empirical(params=tf.Variable(tf.ones([T])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "inference = ed.HMC(latent_vars={weights: q_weights2,\n",
    "                                intercept: q_intercept2,\n",
    "                                var: q_var2},\n",
    "                   data={x: x_train2,\n",
    "                         y: y_train2})\n",
    "inference.run(step_size=0.1 / N1, n_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# however, this doesn't work!\n",
    "# empirical distributions are not admissible to HMC!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's do this with KLqp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL FOR 1st BATCH\n",
    "x = tf.placeholder(tf.float32, [N1, D])\n",
    "weights = Normal(loc=tf.zeros(D), scale=tf.ones(D))\n",
    "intercept = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "var = InverseGamma(concentration=5.0, rate=1.0)\n",
    "y = Normal(loc=ed.dot(x, weights) + intercept, scale=tf.ones(N1) * tf.sqrt(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL FOR 1st BATCH\n",
    "q_weights1 = Normal(loc=tf.Variable(tf.random_normal([D])),\n",
    "                    scale=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\n",
    "q_intercept1 = Normal(loc=tf.Variable(tf.random_normal([1])),\n",
    "                      scale=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))\n",
    "q_var1 = InverseGamma(concentration=tf.nn.softplus(tf.Variable(tf.random_normal([]))),\n",
    "                      rate=tf.nn.softplus(tf.Variable(tf.random_normal([]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE FOR 1st BATCH\n",
    "inference = ed.KLqp(latent_vars={weights: q_weights1,\n",
    "                                 intercept: q_intercept1,\n",
    "                                 var: q_var1},\n",
    "                    data={x: x_train1,\n",
    "                          y: y_train1})\n",
    "inference.run(n_samples=50, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM FOR 1st BATCH\n",
    "plt.scatter(x_train1, y_train1, s=20.0);  # blue\n",
    "plt.scatter(x_test, y_test, s=20.0,\n",
    "            color=sns.color_palette().as_hex()[2]);  # red\n",
    "\n",
    "xp = tf.placeholder(tf.float32, [2, D])\n",
    "[plt.plot(np.linspace(-4.0, 4.0, 2),\n",
    "          sess.run(ed.dot(xp, q_weights1) + q_intercept1,\n",
    "                   {xp: np.linspace(-4.0, 4.0, 2)[:, np.newaxis]}),\n",
    "          color='black', alpha=0.1)\n",
    " for _ in range(50)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_post = ed.copy(y, {weights: q_weights1,\n",
    "                     intercept: q_intercept1,\n",
    "                     var: q_var1})\n",
    "# this is equivalent to\n",
    "# y_post = Normal(loc=ed.dot(x, q_weights1) + q_intercept1,\n",
    "#                 scale=tf.ones(N1) * tf.sqrt(q_var1))\n",
    "# ed.copy works for us only because Np=N1!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={x: x_test, y_post: y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={x: x_test, y_post: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL FOR 2nd BATCH\n",
    "x = tf.placeholder(tf.float32, [N2, D])\n",
    "weights = q_weights1\n",
    "intercept = q_intercept1\n",
    "var = q_var1\n",
    "y = Normal(loc=ed.dot(x, weights) + intercept, scale=tf.ones(N2) * tf.sqrt(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL FOR 2nd BATCH\n",
    "q_weights2 = Normal(loc=tf.Variable(tf.random_normal([D])),\n",
    "                    scale=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))\n",
    "q_intercept2 = Normal(loc=tf.Variable(tf.random_normal([1])),\n",
    "                      scale=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))\n",
    "q_var2 = InverseGamma(concentration=tf.nn.softplus(tf.Variable(tf.random_normal([]))),\n",
    "                      rate=tf.nn.softplus(tf.Variable(tf.random_normal([]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE FOR 2nd BATCH\n",
    "inference = ed.KLqp(latent_vars={weights: q_weights2,\n",
    "                                 intercept: q_intercept2,\n",
    "                                 var: q_var2},\n",
    "                    data={x: x_train2,\n",
    "                          y: y_train2})\n",
    "inference.run(n_samples=50, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM FOR 2nd BATCH\n",
    "plt.scatter(np.concatenate((x_train1, x_train2)),\n",
    "            np.concatenate((y_train1, y_train2)), s=20.0);  # blue\n",
    "plt.scatter(x_test, y_test, s=20.0,\n",
    "            color=sns.color_palette().as_hex()[2]);  # red\n",
    "\n",
    "xp = tf.placeholder(tf.float32, [2, D])\n",
    "[plt.plot(np.linspace(-4.0, 4.0, 2),\n",
    "          sess.run(ed.dot(xp, q_weights2) + q_intercept2,\n",
    "                   {xp: np.linspace(-4.0, 4.0, 2)[:, np.newaxis]}),\n",
    "          color='black', alpha=0.1)\n",
    " for _ in range(50)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp = tf.placeholder(tf.float32, [Np, D])\n",
    "y_post = Normal(loc=ed.dot(xp, q_weights2) + q_intercept2,\n",
    "                scale=tf.ones(Np) * tf.sqrt(q_var2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={xp: x_test, y_post: y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={xp: x_test, y_post: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For The Adventurous: Bayesian Nonlinear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "N = 1000  # number of training data points\n",
    "Np = 100  # number of test data points\n",
    "D = 1  # number of features\n",
    "\n",
    "weights_true = sess.run(Normal(loc=tf.ones(D) * 1.25,\n",
    "                               scale=tf.ones(D) * 0.1))  # unknown true weights\n",
    "intercept_true = sess.run(Normal(loc=tf.zeros(1),\n",
    "                                 scale=tf.ones(1)))  # unknown true intercept\n",
    "noise_true = 0.1  # unknown true amount of noise\n",
    "\n",
    "def target_function(x):\n",
    "    return tf.sin(tf.square(ed.dot(x, weights_true))) + intercept_true\n",
    "\n",
    "def build_dataset(N):\n",
    "    x = Normal(loc=tf.zeros([N, D]), scale=tf.ones([N, D]))\n",
    "    y = Normal(loc=target_function(x), scale=noise_true)\n",
    "    return sess.run([x, y])\n",
    "\n",
    "x_train, y_train = build_dataset(N)\n",
    "x_test, y_test = build_dataset(Np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(x_train, y_train, s=20.0);\n",
    "plt.scatter(x_test, y_test, s=20.0,\n",
    "            color=sns.color_palette().as_hex()[2]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL A\n",
    "def neural_network_with_2_layers(x, W_0, W_1, b_0, b_1):\n",
    "    h = tf.nn.tanh(tf.matmul(x, W_0) + b_0)\n",
    "    h = tf.matmul(h, W_1) + b_1\n",
    "    return tf.reshape(h, [-1])\n",
    "\n",
    "dim = 10  # layer dimensions\n",
    "\n",
    "W_0 = Normal(loc=tf.zeros([D, dim]),\n",
    "             scale=tf.ones([D, dim]))\n",
    "W_1 = Normal(loc=tf.zeros([dim, 1]),\n",
    "             scale=tf.ones([dim, 1]))\n",
    "b_0 = Normal(loc=tf.zeros(dim),\n",
    "             scale=tf.ones(dim))\n",
    "b_1 = Normal(loc=tf.zeros(1),\n",
    "             scale=tf.ones(1))\n",
    "\n",
    "x = tf.placeholder(tf.float32, [N, D])\n",
    "y = Normal(loc=neural_network_with_2_layers(x, W_0, W_1, b_0, b_1),\n",
    "           scale=tf.ones(N) * 0.1)  # constant noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL A\n",
    "q_W_0 = Normal(loc=tf.Variable(tf.random_normal([D, dim])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([D, dim]))))\n",
    "q_W_1 = Normal(loc=tf.Variable(tf.random_normal([dim, 1])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([dim, 1]))))\n",
    "q_b_0 = Normal(loc=tf.Variable(tf.random_normal([dim])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([dim]))))\n",
    "q_b_1 = Normal(loc=tf.Variable(tf.random_normal([1])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE A\n",
    "# this will take a couple of minutes\n",
    "inference = ed.KLqp(latent_vars={W_0: q_W_0, b_0: q_b_0,\n",
    "                                 W_1: q_W_1, b_1: q_b_1},\n",
    "                    data={x: x_train, y: y_train})\n",
    "inference.run(n_samples=10, n_iter=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM A\n",
    "plt.scatter(x_train, y_train, s=20.0);  # blue\n",
    "plt.scatter(x_test, y_test, s=20.0,  # red\n",
    "            color=sns.color_palette().as_hex()[2]);\n",
    "\n",
    "xp = tf.placeholder(tf.float32, [1000, D])\n",
    "[plt.plot(np.linspace(-4.0, 4.0, 1000),\n",
    "          sess.run(neural_network_with_2_layers(xp,\n",
    "                                                q_W_0, q_W_1,\n",
    "                                                q_b_0, q_b_1),\n",
    "                   {xp: np.linspace(-4.0, 4.0, 1000)[:, np.newaxis]}),\n",
    "          color='black', alpha=0.1)\n",
    " for _ in range(50)];\n",
    "\n",
    "plt.plot(np.linspace(-4.0, 4.0, 1000),\n",
    "         sess.run(target_function(xp),  # blue\n",
    "                  {xp: np.linspace(-4.0, 4.0, 1000)[:, np.newaxis]}));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp = tf.placeholder(tf.float32, [Np, D])\n",
    "y_post = Normal(loc=neural_network_with_2_layers(xp,\n",
    "                                                 q_W_0, q_W_1,\n",
    "                                                 q_b_0, q_b_1),\n",
    "                scale=tf.ones(Np) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={xp: x_test, y_post: y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={xp: x_test, y_post: y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL B\n",
    "def neural_network_with_3_layers(x, W_0, W_1, W_2, b_0, b_1, b_2):\n",
    "    h = tf.tanh(tf.matmul(x, W_0) + b_0)\n",
    "    h = tf.tanh(tf.matmul(h, W_1) + b_1)\n",
    "    h = tf.matmul(h, W_2) + b_2\n",
    "    return tf.reshape(h, [-1])\n",
    "\n",
    "dim = 10  # layer dimensions\n",
    "\n",
    "W_0 = Normal(loc=tf.zeros([D, dim]),\n",
    "             scale=tf.ones([D, dim]))\n",
    "W_1 = Normal(loc=tf.zeros([dim, dim]),\n",
    "             scale=tf.ones([dim, dim]))\n",
    "W_2 = Normal(loc=tf.zeros([dim, 1]),\n",
    "             scale=tf.ones([dim, 1]))\n",
    "b_0 = Normal(loc=tf.zeros(dim),\n",
    "             scale=tf.ones(dim))\n",
    "b_1 = Normal(loc=tf.zeros(dim),\n",
    "             scale=tf.ones(dim))\n",
    "b_2 = Normal(loc=tf.zeros(1),\n",
    "             scale=tf.ones(1))\n",
    "\n",
    "x = tf.placeholder(tf.float32, [N, D])\n",
    "y = Normal(loc=neural_network_with_3_layers(x, W_0, W_1, W_2, b_0, b_1, b_2),\n",
    "           scale=tf.ones(N) * 0.1)  # constant noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL B\n",
    "q_W_0 = Normal(loc=tf.Variable(tf.random_normal([D, dim])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([D, dim]))))\n",
    "q_W_1 = Normal(loc=tf.Variable(tf.random_normal([dim, dim])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([dim, dim]))))\n",
    "q_W_2 = Normal(loc=tf.Variable(tf.random_normal([dim, 1])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([dim, 1]))))\n",
    "q_b_0 = Normal(loc=tf.Variable(tf.random_normal([dim])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([dim]))))\n",
    "q_b_1 = Normal(loc=tf.Variable(tf.random_normal([dim])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([dim]))))\n",
    "q_b_2 = Normal(loc=tf.Variable(tf.random_normal([1])),\n",
    "               scale=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE B\n",
    "# this will take ten minutes or longer\n",
    "inference = ed.KLqp(latent_vars={W_0: q_W_0, b_0: q_b_0,\n",
    "                                 W_1: q_W_1, b_1: q_b_1,\n",
    "                                 W_2: q_W_2, b_2: q_b_2},\n",
    "                    data={x: x_train, y: y_train})\n",
    "inference.run(n_samples=10, n_iter=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM B\n",
    "plt.scatter(x_train, y_train, s=20.0);  # blue\n",
    "plt.scatter(x_test, y_test, s=20.0,  # red\n",
    "            color=sns.color_palette().as_hex()[2]);\n",
    "\n",
    "xp = tf.placeholder(tf.float32, [1000, D])\n",
    "[plt.plot(np.linspace(-4.0, 4.0, 1000),\n",
    "          sess.run(neural_network_with_3_layers(xp,\n",
    "                                                q_W_0, q_W_1, q_W_2,\n",
    "                                                q_b_0, q_b_1, q_b_2),\n",
    "                   {xp: np.linspace(-4.0, 4.0, 1000)[:, np.newaxis]}),\n",
    "          color='black', alpha=0.1)\n",
    " for _ in range(50)];\n",
    "\n",
    "plt.plot(np.linspace(-4.0, 4.0, 1000),\n",
    "         sess.run(target_function(xp),  # blue\n",
    "                  {xp: np.linspace(-4.0, 4.0, 1000)[:, np.newaxis]}));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xp = tf.placeholder(tf.float32, [Np, D])\n",
    "y_post = Normal(loc=neural_network_with_3_layers(xp,\n",
    "                                                 q_W_0, q_W_1, q_W_2,\n",
    "                                                 q_b_0, q_b_1, q_b_2),\n",
    "                scale=tf.ones(Np) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={xp: x_test, y_post: y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={xp: x_test, y_post: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-sided Die"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Categorical, Dirichlet, Empirical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(N, K, fairness):\n",
    "    probs = Dirichlet(concentration=tf.ones([K]) * fairness)\n",
    "    c = Categorical(probs=probs, sample_shape=N)\n",
    "    return sess.run([probs, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA: 500D3\n",
    "N = 500  # number of data points (dice rolls)\n",
    "K = 3  # number of components (sides of the die)\n",
    "fairness = 1.0  # the larger the fairer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs_true, c_train = build_dataset(N, K, fairness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(probs_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL\n",
    "probs = Dirichlet(concentration=tf.ones([K]))  # generalization of Beta\n",
    "c = Categorical(probs=probs, sample_shape=N)  # generalization of Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "probs_cond = ed.complete_conditional(probs)\n",
    "probs_post = ed.copy(probs_cond, {c: c_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the posterior is again a Dirichlet\n",
    "sess.run({key: val for\n",
    "          key, val in six.iteritems(probs_post.parameters)\n",
    "          if isinstance(val, tf.Tensor)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "x, y = zip(*probs_post.sample(2000)[:,:2].eval())\n",
    "sns.kdeplot(np.array(x), np.array(y), shade=True, shade_lowest=False)\n",
    "plt.axvline(x=probs_true[0]);  # blue\n",
    "plt.axhline(y=probs_true[1], c=sns.color_palette()[2]);  # red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Semisupervised Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import (Categorical, Dirichlet, InverseGamma,\n",
    "                           ParamMixture, MultivariateNormalDiag,\n",
    "                           Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(N, Np, probs, means, vars):\n",
    "    c = Categorical(probs=tf.tile(tf.reshape(probs, [1, K]),\n",
    "                                  [N + Np, 1]))\n",
    "    x = Normal(loc=tf.gather(means, c),\n",
    "               scale=tf.gather(tf.sqrt(vars), c))\n",
    "    class_samples, feature_samples = sess.run([c, x])\n",
    "    ((c_samples, cp_samples),\n",
    "     (x_samples, xp_samples)) = map(lambda l: (l[:N], l[N:]),\n",
    "                                    sess.run([c, x]))\n",
    "    return [c_samples, x_samples, cp_samples, xp_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "N = 100  # number of labelled data points\n",
    "Np = 1000  # number of unlabelled data points\n",
    "K = 3  # number of classes\n",
    "D = 2  # number of features\n",
    "\n",
    "(c_train, x_train, cp_true, xp_train) = build_dataset(\n",
    "    N, Np,\n",
    "    probs=[0.65, 0.1, 0.25],\n",
    "    means=[[-1.0, 0.0], [1.0, 0.5], [0.5, -1.0]], \n",
    "    vars=[[1.0, 5.0], [0.25, 20.], [3.0, 2.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL\n",
    "class_concentration = 1.0\n",
    "loc = 0.\n",
    "var_concentration = 0.5\n",
    "var_rate = 0.5\n",
    "sample_size = 1.0\n",
    "\n",
    "probs = Dirichlet(concentration=tf.ones([K]) * class_concentration)  # class prevalence\n",
    "var = InverseGamma(concentration=tf.ones([K, D]) * var_concentration,\n",
    "                   rate=tf.ones([K, D]) * var_rate)  # variance of features\n",
    "mean = Normal(loc=tf.ones([K, D]) * loc,\n",
    "              # scale=tf.sqrt(var) / tf.sqrt(sample_size))\n",
    "              scale=tf.ones([K, D]))  # means of features\n",
    "\n",
    "x = ParamMixture(mixing_weights=probs,\n",
    "                 component_params={'loc': mean,\n",
    "                                   'scale_diag': tf.sqrt(var)},\n",
    "                 component_dist=MultivariateNormalDiag,\n",
    "                 sample_shape=N)  # features for labelled observations\n",
    "c = x.cat  # class labels for labelled observations\n",
    "\n",
    "xp = ParamMixture(mixing_weights=probs,\n",
    "                  component_params={'loc': mean,\n",
    "                                    'scale_diag': tf.sqrt(var)},\n",
    "                  component_dist=MultivariateNormalDiag,\n",
    "                  sample_shape=Np)  # features for unlabelled observations\n",
    "cp = xp.cat  # class labels for unlabelled observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "T = 10000  # number of empirical samples\n",
    "\n",
    "q_probs = Empirical(params=tf.Variable(tf.ones([T, K]) / K))\n",
    "q_var = Empirical(params=tf.Variable(tf.ones([T, K, D])))\n",
    "q_mean = Empirical(params=tf.Variable(tf.zeros([T, K, D])))\n",
    "q_cp = Empirical(params=tf.Variable(tf.zeros([T, Np], dtype=tf.int32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "inference = ed.Gibbs(latent_vars={probs: q_probs,\n",
    "                                  var: q_var,\n",
    "                                  mean: q_mean,\n",
    "                                  cp: q_cp},\n",
    "                     data={c: c_train,\n",
    "                           x: x_train,\n",
    "                           xp: xp_train})\n",
    "inference.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim=(-4.5, 3), ylim=(-15, 15))\n",
    "for k in range(K):\n",
    "    q_mean_samples = tf.reshape(tf.strided_slice(q_mean.params.eval(),\n",
    "                                                 [int(T/2), k, 0],\n",
    "                                                 [T, K, 2],\n",
    "                                                 [1, K, 1]),\n",
    "                                [int(T/2), 2]).eval()\n",
    "    cmap = sns.light_palette(color=sns.color_palette().as_hex()[k], as_cmap=True)\n",
    "    sns.kdeplot(q_mean_samples[:, 0], q_mean_samples[:, 1],\n",
    "                cmap=cmap, shade=True, shade_lowest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inferred classes for xp data\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim=(-4.5, 3), ylim=(-15, 15))\n",
    "scat = ax.scatter(xp_train[:, 0], xp_train[:, 1], s=20.0)\n",
    "\n",
    "scat.set_array(q_cp.params[-1].eval())  # use the last sample\n",
    "scat.set_cmap(ListedColormap(sns.color_palette().as_hex()))\n",
    "scat.set_norm(Normalize(vmin=0, vmax=len(sns.color_palette())-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# true classes for xp data\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim=(-4.5, 3), ylim=(-15, 15))\n",
    "scat = ax.scatter(xp_train[:, 0], xp_train[:, 1], s=20.0)\n",
    "\n",
    "scat.set_array(cp_true)  # use the true class label\n",
    "scat.set_cmap(ListedColormap(sns.color_palette().as_hex()))\n",
    "scat.set_norm(Normalize(vmin=0, vmax=len(sns.color_palette())-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ed.criticisms.binary_accuracy(cp_true, q_cp).eval()\n",
    "ed.criticisms.binary_accuracy(cp_true, q_cp.params[int(T/2):]).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this takes a couple of minutes\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(xlim=(-4.5, 3), ylim=(-15, 15))\n",
    "scat = ax.scatter(xp_train[:, 0], xp_train[:, 1], s=20.0)\n",
    "\n",
    "def init():\n",
    "    scat.set_array(q_cp.params[0].eval())\n",
    "    scat.set_cmap(ListedColormap(sns.color_palette().as_hex()))\n",
    "    scat.set_norm(Normalize(vmin=0, vmax=len(sns.color_palette())-1))\n",
    "    return (scat,)\n",
    "\n",
    "def animate(i):\n",
    "    scat.set_array(q_cp.params[i].eval())\n",
    "    return (scat,)\n",
    "\n",
    "anim = FuncAnimation(fig, animate, init_func=init,\n",
    "                     frames=range(1, 100), interval=20, blit=True)\n",
    "\n",
    "display_animation(anim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter(lambda xp:   0.5 < xp[0][0] < 1.0 and\n",
    "                  -0.25 < xp[0][1] < 0.25,\n",
    "       zip(xp_train, range(len(xp_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.countplot(q_cp.params[int(T/2):, 0].eval(), order=range(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For The Adventurous: Bayesian Probabilistic Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Bernoulli, Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataset(U, V, N, M, noise=0.1):\n",
    "    loc = tf.matmul(tf.transpose(U), V)\n",
    "    R = Normal(loc=loc, scale=tf.ones(loc.shape) * noise)\n",
    "    return R\n",
    "\n",
    "def build_indicators(N, M, prob=0.5):\n",
    "    I = Bernoulli(probs=tf.ones([N, M]) * prob)\n",
    "    return I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA\n",
    "N = 50  # number of users\n",
    "M = 60  # number of movies\n",
    "D = 3  # number of latent factors\n",
    "\n",
    "# true latent factors\n",
    "U_true = Normal(loc=tf.zeros([D, N]), scale=tf.ones([D, N])).eval()\n",
    "V_true = Normal(loc=tf.zeros([D, M]), scale=tf.ones([D, M])).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_true = build_dataset(U_true, V_true, N, M).eval()\n",
    "I_train = build_indicators(N, M).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FORWARD MODEL\n",
    "I = tf.placeholder(tf.bool, [N, M])\n",
    "U = Normal(loc=tf.zeros([D, N]), scale=tf.ones([D, N]))\n",
    "V = Normal(loc=tf.zeros([D, M]), scale=tf.ones([D, M]))\n",
    "loc = tf.matmul(tf.transpose(U), V)\n",
    "scale = tf.ones([N, M])\n",
    "R_obs = Normal(loc=tf.boolean_mask(loc, I),\n",
    "               scale=tf.boolean_mask(scale, I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BACKWARD MODEL\n",
    "q_U = Normal(loc=tf.Variable(tf.random_normal([D, N])),\n",
    "             scale=tf.nn.softplus(tf.Variable(tf.random_normal([D, N]))))\n",
    "q_V = Normal(loc=tf.Variable(tf.random_normal([D, M])),\n",
    "             scale=tf.nn.softplus(tf.Variable(tf.random_normal([D, M]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "inference = ed.KLqp(latent_vars={U: q_U,\n",
    "                                 V: q_V},\n",
    "                    data={R_obs: tf.boolean_mask(R_true.astype(np.float32),\n",
    "                                                 I_train.astype(bool)),\n",
    "                          I: I_train.astype(bool)})\n",
    "inference.run(n_samples=10, n_iter=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CRITICISM\n",
    "q_loc = tf.matmul(tf.transpose(q_U), q_V)\n",
    "R_mis = Normal(loc=tf.boolean_mask(q_loc, tf.logical_not(I)),\n",
    "               scale=tf.boolean_mask(scale, tf.logical_not(I)))\n",
    "\n",
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error',\n",
    "                  data={R_mis: tf.boolean_mask(R_true.astype(np.float32),\n",
    "                                               tf.logical_not(I_train.astype(bool))),\n",
    "                        I: I_train.astype(bool)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(R_true,\n",
    "           cmap=sns.diverging_palette(240, 10, n=9, as_cmap=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R_est = tf.matmul(tf.transpose(q_U), q_V).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(R_est,\n",
    "           cmap=sns.diverging_palette(240, 10, n=9, as_cmap=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(Normal(loc=tf.matmul(tf.transpose(q_U), q_V),\n",
    "                  scale=scale).mean().eval(),\n",
    "           cmap=sns.diverging_palette(240, 10, n=9, as_cmap=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 (PyCon)",
   "language": "python",
   "name": "python2_pycon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
